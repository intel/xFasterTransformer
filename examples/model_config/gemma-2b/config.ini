[gemma]
model_name = /data/models/gemma-2b-it
head_num = 8
kv_head_num = 1
hidden_size = 2048
size_per_head = 256
inter_size = 16384
max_pos_seq_len = 8192
num_layer = 18
layernorm_eps = 1e-06
layernorm_type = pre_layernorm
activation_type = gelu
rope_theta = 10000.0
scaling_factor = 1.0
rope_type = null
has_post_decoder_layernorm = 1
vocab_size = 256000
start_id = 2
end_id = 1
pad_id = 0
weight_data_type = fp16

