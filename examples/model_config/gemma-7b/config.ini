[gemma]
model_name = /data/models/gemma-7b-it
head_num = 16
kv_head_num = 16
hidden_size = 3072
size_per_head = 256
inter_size = 24576
max_pos_seq_len = 8192
num_layer = 28
layernorm_eps = 1e-06
layernorm_type = pre_layernorm
activation_type = gelu
rope_theta = 10000.0
scaling_factor = 1.0
rope_type = null
has_post_decoder_layernorm = 1
vocab_size = 256000
start_id = 2
end_id = 1
pad_id = 0
weight_data_type = fp16

