[chatglm2]
model_name = /data/chatglm2-6b-hf/
head_num = 32
size_per_head = 128
inter_size = 13696
max_pos_seq_len = 32768
num_layer = 28
layernorm_eps = 1e-05
layernorm_type = pre_layernorm
activation_type = swiglu
has_post_decoder_layernorm = 1
vocab_size = 65024
start_id = None
end_id = 2
weight_data_type = fp16
kv_channels = 128
rmsnorm = 1
apply_residual_connection_post_layernorm = 0
multi_query_attention = 1
kv_head_num = 2
pad_id = None

