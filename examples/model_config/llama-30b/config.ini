[llama]
model_name = /data/models/llama-30b/
head_num = 52
kv_head_num = 52
size_per_head = 128
inter_size = 17920
max_pos_seq_len = 2048
num_layer = 60
rms_norm_eps = 1e-6
layernorm_type = pre_layernorm
activation_type = silu
has_post_decoder_layernorm = 1
vocab_size = 32000
start_id = 0
end_id = 1
weight_data_type = fp16

