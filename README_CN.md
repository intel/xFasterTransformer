# xFasterTransformer

<p align="center">
  <a href="./README.md">English</a> |
  <a href="./README_CN.md">ç®€ä½“ä¸­æ–‡</a>
</p>

xFasterTransformerä¸ºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨CPU X86å¹³å°ä¸Šçš„éƒ¨ç½²æä¾›äº†ä¸€ç§æ·±åº¦ä¼˜åŒ–çš„è§£å†³æ–¹æ¡ˆï¼Œæ”¯æŒå¤šCPUèŠ‚ç‚¹ä¹‹é—´çš„åˆ†å¸ƒå¼éƒ¨ç½²æ–¹æ¡ˆï¼Œä½¿å¾—è¶…å¤§æ¨¡å‹åœ¨CPUä¸Šçš„éƒ¨ç½²æˆä¸ºå¯èƒ½ã€‚æ­¤å¤–ï¼ŒxFasterTransformeræä¾›äº†C++å’ŒPythonä¸¤ç§APIæ¥å£ï¼Œæ¶µç›–äº†ä»ä¸Šå±‚åˆ°åº•å±‚çš„æ¥å£è°ƒç”¨ï¼Œæ˜“äºç”¨æˆ·ä½¿ç”¨å¹¶å°†xFasterTransformeré›†æˆåˆ°è‡ªæœ‰ä¸šåŠ¡æ¡†æ¶ä¸­ã€‚

## æ–°é—»ğŸ”¥
-  xFasterTransformer ç°åœ¨æ”¯æŒ Qwen3 ç³»åˆ—æ¨¡å‹å•¦ï¼
-  [xFastertransformeræ”¯æŒæ»¡è¡€ç‰ˆ671B DeepSeek-R1å•¦ï¼ç‚¹å‡»äº†è§£è¯¦æƒ…ã€‚](docs/zh/deepseek.md)

## ç›®å½•
- [xFasterTransformer](#xfastertransformer)
  - [ç›®å½•](#ç›®å½•)
  - [æ¨¡å‹æ¦‚è§ˆ](#æ¨¡å‹æ¦‚è§ˆ)
    - [æ”¯æŒçš„æ¨¡å‹](#æ”¯æŒçš„æ¨¡å‹)
    - [æ”¯æŒçš„æ•°æ®ç±»å‹](#æ”¯æŒçš„æ•°æ®ç±»å‹)
  - [ç›®å½•](#ç›®å½•-1)
  - [å®‰è£…](#å®‰è£…)
    - [ä½¿ç”¨ PyPI](#ä½¿ç”¨-pypi)
    - [ä½¿ç”¨ Docker](#ä½¿ç”¨-docker)
    - [ä»æºç æ„å»º](#ä»æºç æ„å»º)
      - [å‡†å¤‡ç¯å¢ƒ](#å‡†å¤‡ç¯å¢ƒ)
        - [æ‰‹åŠ¨æ“ä½œ](#æ‰‹åŠ¨æ“ä½œ)
        - [å®‰è£…ä¾èµ–çš„åº“](#å®‰è£…ä¾èµ–çš„åº“)
        - [å¦‚ä½•ç¼–è¯‘](#å¦‚ä½•ç¼–è¯‘)
  - [æ¨¡å‹å‡†å¤‡](#æ¨¡å‹å‡†å¤‡)
  - [API ç”¨æ³•](#api-ç”¨æ³•)
    - [Python API(PyTorch)](#python-apipytorch)
    - [C++ API](#c-api)
  - [å¦‚ä½•è¿è¡Œ](#å¦‚ä½•è¿è¡Œ)
    - [å•è¿›ç¨‹](#å•è¿›ç¨‹)
    - [å¤šè¿›ç¨‹](#å¤šè¿›ç¨‹)
      - [å‘½ä»¤è¡Œè°ƒç”¨](#å‘½ä»¤è¡Œè°ƒç”¨)
      - [ä»£ç å®ç°](#ä»£ç å®ç°)
        - [Python](#python)
        - [C++](#c)
  - [ç½‘é¡µç¤ºä¾‹](#ç½‘é¡µç¤ºä¾‹)
  - [æœåŠ¡](#æœåŠ¡)
    - [vLLM](#vllm)
      - [Install](#install)
      - [å…¼å®¹OpenAI-APIçš„æœåŠ¡](#å…¼å®¹openai-apiçš„æœåŠ¡)
    - [FastChat](#fastchat)
    - [MLServer](#mlserver)
  - [æ€§èƒ½æµ‹è¯•](#æ€§èƒ½æµ‹è¯•)
  - [æŠ€æœ¯æ”¯æŒ](#æŠ€æœ¯æ”¯æŒ)
  - [è®ºæ–‡å‘è¡¨](#è®ºæ–‡å‘è¡¨)
  - [é—®é¢˜ä¸å›ç­”](#é—®é¢˜ä¸å›ç­”)

## æ¨¡å‹æ¦‚è§ˆ
å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‘å±•é€Ÿåº¦éå¸¸å¿«ï¼Œåœ¨è®¸å¤šäººå·¥æ™ºèƒ½åœºæ™¯ä¸­å¾—åˆ°äº†å¹¿æ³›çš„åº”ç”¨ã€‚xFasterTransformer å……åˆ†åˆ©ç”¨äº†è‡³å¼ºå¹³å°çš„ç¡¬ä»¶èƒ½åŠ›ï¼Œåœ¨å•é¢—CPUå’Œå¤šé¢—CPU/å¤šèŠ‚ç‚¹ä¸Šå®ç°äº† LLM æ¨ç†çš„é«˜æ€§èƒ½å’Œé«˜å¯æ‰©å±•æ€§ã€‚

xFasterTransformer æä¾›äº†ä¸€ç³»åˆ— C++ å’Œ Python åº”ç”¨ç¨‹åºæ¥å£ï¼Œç»ˆç«¯ç”¨æˆ·å¯å°† xFasterTransformer ç›´æ¥é›†æˆåˆ°è‡ªå·±çš„è§£å†³æ–¹æ¡ˆæˆ–æœåŠ¡ä¸­ã€‚æ­¤å¤–ï¼ŒxFTè¿˜æä¾›äº†å¤šç§ç¤ºä¾‹ä»£ç æ¥æ¼”ç¤ºä½¿ç”¨æ–¹æ³•ã€‚åŒ…æ‹¬ä¾›ç”¨æˆ·è¿›è¡Œæ€§èƒ½æµ‹è¯•çš„æµ‹è¯•ä»£ç å’Œè„šæœ¬ï¼Œä»¥åŠé€šè¿‡ç½‘é¡µæ¨¡å¼æ­å»ºå¸¸ç”¨ LLM æ¨¡å‹æœåŠ¡çš„ç¤ºä¾‹ã€‚


### æ”¯æŒçš„æ¨¡å‹

|        æ¨¡å‹         |   æ¡†æ¶   |          | åˆ†å¸ƒå¼æ”¯æŒ   |
| :----------------: | :------: | :------: | :--------: |
|                    | PyTorch  |   C++    |            |
|     DeepSeekR1     | &#10004; | &#10004; |  &#10004;  |
|     DeepSeekV3     | &#10004; | &#10004; |  &#10004;  |
|     DeepSeekV2     | &#10004; | &#10004; |  &#10004;  |
|      ChatGLM       | &#10004; | &#10004; |  &#10004;  |
|      ChatGLM2      | &#10004; | &#10004; |  &#10004;  |
|      ChatGLM3      | &#10004; | &#10004; |  &#10004;  |
|        GLM4        | &#10004; | &#10004; |  &#10004;  |
|       Llama        | &#10004; | &#10004; |  &#10004;  |
|       Llama2       | &#10004; | &#10004; |  &#10004;  |
|       Llama3       | &#10004; | &#10004; |  &#10004;  |
|     Baichuan1      | &#10004; | &#10004; |  &#10004;  |
|     Baichuan2      | &#10004; | &#10004; |  &#10004;  |
|        QWen        | &#10004; | &#10004; |  &#10004;  |
|        QWen2       | &#10004; | &#10004; |  &#10004;  |
|        QWen3       | &#10004; | &#10004; |  &#10004;  |
| SecLLM(YaRN-Llama) | &#10004; | &#10004; |  &#10004;  |
|        Opt         | &#10004; | &#10004; |  &#10004;  |
|   Deepseek-coder   | &#10004; | &#10004; |  &#10004;  |
|       gemma        | &#10004; | &#10004; |  &#10004;  |
|     gemma-1.1      | &#10004; | &#10004; |  &#10004;  |
|     codegemma      | &#10004; | &#10004; |  &#10004;  |
|      TeleChat      | &#10004; | &#10004; |  &#10004;  |
|     Mixtral(MoE)   | &#10004; | &#10004; |  &#10004;  |

### æ”¯æŒçš„æ•°æ®ç±»å‹

- FP16
- BF16
- INT8
- W8A8
- INT4
- NF4
- BF16_FP16
- BF16_INT8
- BF16_W8A8
- BF16_INT4
- BF16_NF4
- W8A8_INT8
- W8A8_int4
- W8A8_NF4

## ç›®å½•
xFasterTransformer æ–‡æ¡£å’Œ[Wiki](https://github.com/intel/xFasterTransformer/wiki)æä¾›äº†ä»¥ä¸‹èµ„æºï¼š
- xFasterTransformer ç®€ä»‹ã€‚
- C++ å’Œ PyTorch ä¸Šå±‚å’Œåº•å±‚æ¥å£çš„å…¨é¢ API å‚è€ƒèµ„æ–™ã€‚
- åœ¨ C++ å’Œ PyTorch ä¸­ä½¿ç”¨ xFasterTransformer çš„å®ç”¨ API ç¤ºä¾‹ã€‚

## å®‰è£…
### ä½¿ç”¨ PyPI
```bash
pip install xfastertransformer
```

### ä½¿ç”¨ Docker
```bash
docker pull intel/xfastertransformer:latest
```
ä½¿ç”¨å‘½ä»¤è¿è¡Œ docker (å‡è®¾æ¨¡å‹æ–‡ä»¶ä½äº `/data/` ç›®å½•):  
```bash
docker run -it \
    --name xfastertransformer \
    --privileged \
    --shm-size=16g \
    -v /data/:/data/ \
    -e "http_proxy=$http_proxy" \
    -e "https_proxy=$https_proxy" \
    intel/xfastertransformer:latest
```
**æ³¨æ„!!!**: å¦‚æœåœ¨å¤šè¿›ç¨‹æ¨¡å¼ä¸‹è¿è¡Œæ—¶å‘ç”Ÿ**bus error**ï¼Œè¯·å¢å¤§"--shm-size"ã€‚dockeré»˜è®¤é™åˆ¶å…±äº«å†…å­˜å¤§å°ä¸º64MBï¼Œè€Œæˆ‘ä»¬çš„å®ç°ä½¿ç”¨å¤§é‡çš„å…±äº«å†…å­˜æ¥è·å¾—æ›´å¥½çš„æ€§èƒ½ã€‚

### ä»æºç æ„å»º
#### å‡†å¤‡ç¯å¢ƒ
##### æ‰‹åŠ¨æ“ä½œ
- [PyTorch](https://pytorch.org/get-started/locally/) v2.3 (ä½¿ç”¨ PyTorch API æ—¶éœ€è¦ï¼Œä½†ä½¿ç”¨ C++ API æ—¶ä¸éœ€è¦ã€‚)
  ```bash 
  pip install torch --index-url https://download.pytorch.org/whl/cpu
  ```

##### å®‰è£…ä¾èµ–çš„åº“

è¯·å®‰è£…æ‰€ä¾èµ–çš„libnumaåº“:
- CentOS: yum install libnuma-devel
- Ubuntu: apt-get install libnuma-dev


##### å¦‚ä½•ç¼–è¯‘
- ä½¿ç”¨ 'CMake'
  ```bash
  # æ„å»º xFasterTransformer
  git clone https://github.com/intel/xFasterTransformer.git xFasterTransformer
  cd xFasterTransformer
  git checkout <latest-tag>
  # å¦‚æœä½¿ç”¨pythonç¤ºä¾‹ï¼Œè¯·ç¡®ä¿å·²ç»å®‰è£…torchã€‚
  mkdir build && cd build
  # æ³¨æ„ä½¿ç”¨gcc-13åŠä»¥ä¸Šç‰ˆæœ¬
  cmake ..
  # è‹¥é‡åˆ°é”™è¯¯ "numa.h: No such file or directory"ï¼Œéœ€è¦å…ˆå®‰è£…numaåŒ…ï¼Œç„¶åä½¿ç”¨ "CPATH=$CONDA_PATH/include/:$CPATH make -j"å®Œæˆç¼–è¯‘
  make -j
  ```
- ä½¿ç”¨ `python setup.py`
  ```bash
  # æ„å»ºBuild xFasterTransformeråº“å’ŒC++ç¤ºä¾‹ã€‚
  python setup.py build

  # å®‰è£…xFastertransformeråˆ°pipç¯å¢ƒä¸­ã€‚
  # æ³¨æ„ï¼šåœ¨å®‰è£…ä¹‹å‰è¯·è¿è¡Œ `python setup.py build`ï¼
  python setup.py install
  ```

## [æ¨¡å‹å‡†å¤‡](tools/README.md)
xFasterTransformer æ”¯æŒçš„æ¨¡å‹æ ¼å¼ä¸ Huggingface æœ‰æ‰€ä¸åŒï¼Œä½†ä¸ FasterTransformer çš„æ ¼å¼å…¼å®¹ã€‚
1. é¦–å…ˆä¸‹è½½ huggingface æ ¼å¼çš„æ¨¡å‹ã€‚
2. ç„¶åï¼Œä½¿ç”¨ xfastertransformer ä¸­çš„æ¨¡å‹è½¬æ¢æ¨¡å—å°†æ¨¡å‹è½¬æ¢ä¸º xFasterTransformer æ ¼å¼ã€‚å¦‚æœæ²¡æœ‰æä¾›è¾“å‡ºç›®å½•ï¼Œè½¬æ¢åçš„æ¨¡å‹å°†è¢«é»˜è®¤æ”¾ç½®åˆ° `${HF_DATASET_DIR}-xft`.
    ```
    python -c "import xfastertransformer as xft; xft.DeepSeekR1Convert().convert('${HF_DATASET_DIR}', '${OUTPUT_DIR}')"
    ```
    ***PS: ç”±äºæ¨¡å‹æ–‡ä»¶å’Œ `transformers` ç‰ˆæœ¬ä¹‹é—´å¯èƒ½å­˜åœ¨å…¼å®¹æ€§é—®é¢˜ï¼Œè¯·é€‰æ‹©ç›¸åº”çš„ `transformers` ç‰ˆæœ¬ã€‚***
    
    æ”¯æŒçš„æ¨¡å‹è½¬æ¢åˆ—è¡¨ï¼š
    - DeepSeekR1Convert
    - DeepSeekV3Convert
    - DeepSeekV2Convert
    - LlamaConvert
    - YiConvert
    - GemmaConvert
    - ChatGLMConvert
    - ChatGLM2Convert
    - ChatGLM3Convert
    - ChatGLM4Convert
    - OPTConvert
    - BaichuanConvert
    - QwenConvert
    - Qwen2Convert
    - Qwen3Convert
    - DeepseekConvert
    - TelechatConvert
    - MixtralConvert

## API ç”¨æ³•
æ›´å¤šè¯¦æƒ…ï¼Œè¯·å‚é˜… API æ–‡æ¡£å’Œ [ç¤ºä¾‹](examples/README.md).
### Python API(PyTorch)
é¦–å…ˆï¼Œè¯·å®‰è£…ä¾èµ–é¡¹ã€‚
- Python ä¾èµ–é¡¹
  ```
  cmake==3.26.1
  sentencepiece==0.2.0
  torch==2.7.0+cpu
  transformers==4.50.0
  accelerate==1.5.1
  protobuf==5.29.3
  tiktoken==0.9.0
  ```
  ***PS: ç”±äºæ¨¡å‹æ–‡ä»¶å’Œ `transformers`ç‰ˆæœ¬ä¹‹é—´å¯èƒ½å­˜åœ¨å…¼å®¹æ€§é—®é¢˜ï¼Œè¯·é€‰æ‹©é€‚å½“çš„ `transformers`ç‰ˆæœ¬ã€‚***
- oneCCL (ç”¨äºå¤šè¿›ç¨‹)  
  å®‰è£… oneCCL å¹¶è®¾ç½®ç¯å¢ƒã€‚è¯·å‚é˜…[å‡†å¤‡ç¯å¢ƒ](#prepare-environment).


xFasterTransformer çš„ Python API ä¸transformersç±»ä¼¼ï¼Œä¹Ÿæ”¯æŒtransformersçš„streameræ¥å®ç°æµå¼è¾“å‡ºã€‚åœ¨ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨transformerså°†è¾“å…¥æ–‡å­—è¿›è¡Œç¼–ç ï¼Œç”Ÿæˆtoken idã€‚
```Python
import xfastertransformer
from transformers import AutoTokenizer, TextStreamer
# å‡è®¾huggingfaceæ ¼å¼çš„æ¨¡å‹ç›®å½•ä¸º`/data/chatglm-6b-hf`ï¼Œè½¬æ¢åæ¨¡å‹çš„ç›®å½•ä¸º`/data/chatglm-6b-xft`.
MODEL_PATH="/data/chatglm-6b-xft"
TOKEN_PATH="/data/chatglm-6b-hf"

INPUT_PROMPT = "Once upon a time, there existed a little girl who liked to have adventures."
tokenizer = AutoTokenizer.from_pretrained(TOKEN_PATH, use_fast=False, padding_side="left", trust_remote_code=True)
streamer = TextStreamer(tokenizer, skip_special_tokens=True, skip_prompt=False)

input_ids = tokenizer(INPUT_PROMPT, return_tensors="pt", padding=False).input_ids
model = xfastertransformer.AutoModel.from_pretrained(MODEL_PATH, dtype="bf16")
generated_ids = model.generate(input_ids, max_length=200, streamer=streamer)
```

### C++ API
[SentencePiece](https://github.com/google/sentencepiece) å¯ç”¨äºæ–‡æœ¬ç¼–ç å’Œè§£ç ã€‚
```C++
#include <vector>
#include <iostream>
#include "xfastertransformer.h"
// ChatGLM token ids for prompt "Once upon a time, there existed a little girl who liked to have adventures."
std::vector<int> input(
        {3393, 955, 104, 163, 6, 173, 9166, 104, 486, 2511, 172, 7599, 103, 127, 17163, 7, 130001, 130004});

// å‡è®¾è½¬æ¢åçš„æ¨¡å‹ç›®å½•ä¸º`/data/chatglm-6b-xft`.
xft::AutoModel model("/data/chatglm-6b-xft", xft::DataType::bf16);

model.config(/*max length*/ 100, /*num beams*/ 1);
model.input(/*input token ids*/ input, /*batch size*/ 1);

while (!model.isDone()) {
    std::vector<int> nextIds = model.generate();
}

std::vector<int> result = model.finalize();
for (auto id : result) {
    std::cout << id << " ";
}
std::cout << std::endl;
```

## å¦‚ä½•è¿è¡Œ
å»ºè®®é¢„åŠ è½½ `libiomp5.so` ä»¥è·å¾—æ›´å¥½çš„æ€§èƒ½ã€‚
- **[æ¨è]** å¦‚æœå·²å®‰è£… xfastertransformer çš„ Python wheel åŒ…ï¼Œè¯·è¿è¡Œ `export $(python -c 'import xfastertransformer as xft; print(xft.get_env())')`ã€‚
- å¦‚æœä»æºä»£ç æ„å»º xFasterTransformerï¼ŒæˆåŠŸæ„å»ºå `libiomp5.so` æ–‡ä»¶å°†åœ¨ `3rdparty/mkl/lib` ç›®å½•ä¸‹ã€‚

### å•è¿›ç¨‹
xFasterTransformer ä¼šè‡ªåŠ¨æ£€æŸ¥ MPI ç¯å¢ƒï¼Œæˆ–è€…ä½¿ç”¨ `SINGLE_INSTANCE=1` ç¯å¢ƒå˜é‡å¼ºåˆ¶åœç”¨ MPIã€‚ 

### å¤šè¿›ç¨‹
#### å‘½ä»¤è¡Œè°ƒç”¨
ä½¿ç”¨ MPI åœ¨å¤šè¿›ç¨‹æ¨¡å¼ä¸‹è¿è¡Œï¼Œè¯·å…ˆå®‰è£… oneCCLã€‚
- [oneCCL å®‰è£…](https://github.com/oneapi-src/oneCCL)
  - å¦‚æœæ‚¨ä»æºä»£ç ç¼–è¯‘äº† xfastertransformerï¼Œåˆ™åœ¨ç¼–è¯‘æ—¶ä¼šåœ¨3rdpartyç›®å½•å®‰è£… oneCCLã€‚
    ```
    source ./3rdparty/oneccl/build/_install/env/setvars.sh
    ```
  - ***[æ¨è]*** ä½¿ç”¨æä¾›çš„è„šæœ¬ä»æºä»£ç ä¸­æ„å»ºã€‚
    ```bash
    cd 3rdparty
    sh prepare_oneccl.sh
    source ./oneccl/build/_install/env/setvars.sh
    ```
  - é€šè¿‡ [IntelÂ® oneAPI Base Toolkit](https://www.intel.com/content/www/us/en/developer/tools/oneapi/base-toolkit-download.html)å®‰è£… oneCCLã€‚***(æ³¨æ„ï¼šå»ºè®®ä½¿ç”¨ 2023.x åŠä»¥ä¸‹ç‰ˆæœ¬ã€‚)*** å¹¶é€šè¿‡ä»¥ä¸‹æ–¹å¼æä¾›ç¯å¢ƒ:
    ```
    source /opt/intel/oneapi/setvars.sh
    ```

- ä¸‹é¢æ˜¯ä¸€ä¸ªæœ¬åœ°ç¯å¢ƒçš„è¿è¡Œæ–¹å¼ç¤ºä¾‹ã€‚ 
  ```bash
  # æˆ–è€…æ‰‹åŠ¨é¢„åŠ è½½ export LD_PRELOAD=libiomp5.so
  export $(python -c 'import xfastertransformer as xft; print(xft.get_env())')
  OMP_NUM_THREADS=48 mpirun \
    -n 1 numactl -N 0  -m 0 ${RUN_WORKLOAD} : \
    -n 1 numactl -N 1  -m 1 ${RUN_WORKLOAD} 
  ```

#### ä»£ç å®ç°
æ›´å¤šè¯¦æƒ…ï¼Œè¯·å‚é˜…ç¤ºä¾‹ã€‚
##### Python
`model.rank` å¯ä»¥è·å¾—è¿›ç¨‹çš„ç¼–å·ï¼Œ`model.rank == 0` æ˜¯ä¸»è¿›ç¨‹ã€‚ 
å¯¹äºä»å±è¿›ç¨‹ï¼ŒåŠ è½½æ¨¡å‹ååªéœ€è¦åš `model.generate()`ã€‚è¾“å…¥å’Œç”Ÿæˆé…ç½®å°†è‡ªåŠ¨åŒæ­¥ã€‚
```Python
model = xfastertransformer.AutoModel.from_pretrained("/data/chatglm-6b-xft", dtype="bf16")

# Slave
while True:
    model.generate()
```
##### C++
`model.getRank()`å¯ä»¥è·å–è¿›ç¨‹çš„ç¼–å·ï¼Œ`model.getRank() == 0` æ˜¯ä¸»è¿›ç¨‹ã€‚ 
å¯¹äºä»å±è¿›ç¨‹ï¼Œå¯ä»¥å‘ `model.config()` å’Œ `model.input` è¾“å…¥ä»»ä½•å€¼ï¼Œå› ä¸ºä¸»è¿›ç¨‹çš„å€¼å°†è¢«åŒæ­¥ã€‚
```C++
xft::AutoModel model("/data/chatglm-6b-xft", xft::DataType::bf16);

// Slave
while (1) {
    model.config();
    std::vector<int> input_ids;
    model.input(/*input token ids*/ input_ids, /*batch size*/ 1);

    while (!model.isDone()) {
        model.generate();
    }
}
```

## [ç½‘é¡µç¤ºä¾‹](examples/web_demo/README.md)
æœ¬ä»“åº“ä¸­æä¾›äº†åŸºäº [Gradio](https://www.gradio.app/)çš„ç½‘é¡µdemoã€‚ç°åœ¨æ”¯æŒ ChatGLMã€ChatGLM2 å’Œ Llama2 æ¨¡å‹ã€‚
- [å‡†å¤‡æ¨¡å‹](#prepare-model).
- å®‰è£…ä¾èµ–é¡¹
  ```bash
  pip install -r examples/web_demo/requirements.txt
  ```
  ***PS: ç”±äºæ¨¡å‹æ–‡ä»¶å’Œ `transformers`ç‰ˆæœ¬ä¹‹é—´å¯èƒ½å­˜åœ¨å…¼å®¹æ€§é—®é¢˜ï¼Œè¯·é€‰æ‹©é€‚å½“çš„ `transformers`ç‰ˆæœ¬ã€‚***
- è¿è¡Œä¸æ¨¡å‹ç›¸å¯¹åº”çš„è„šæœ¬ã€‚ç½‘ç»œæœåŠ¡å™¨å¯åŠ¨åï¼Œåœ¨æµè§ˆå™¨ä¸­æ‰“å¼€è¾“å‡º URL ä»¥ä½¿ç”¨æ¼”ç¤ºç¨‹åºã€‚è¯·æŒ‡å®šæ¨¡å‹å’Œtokenizerç›®å½•çš„è·¯å¾„ä»¥åŠæ•°æ®ç±»å‹ã€‚`transformer`çš„tokenizerç”¨äºå¯¹æ–‡æœ¬è¿›è¡Œç¼–ç å’Œè§£ç ï¼Œå› æ­¤`${TOKEN_PATH}`æŒ‡çš„æ˜¯ huggingface æ¨¡å‹ç›®å½•ã€‚æ­¤æ¼”ç¤ºè¿˜æ”¯æŒå¤šè¿›ç¨‹ã€‚
```bash
# æ¨èé¢„åŠ è½½`libiomp5.so`æ¥è·å¾—æ›´å¥½çš„æ€§èƒ½ã€‚
# `libiomp5.so`æ–‡ä»¶ä¼šä½äºç¼–è¯‘å`3rdparty/mklml/lib`æ–‡ä»¶å¤¹ä¸­ã€‚
# æˆ–è€…æ‰‹åŠ¨é¢„åŠ è½½LD_PRELOAD=libiomp5.so manually, `libiomp5.so`æ–‡ä»¶ä¼šä½äºç¼–è¯‘å`3rdparty/mkl/lib`æ–‡ä»¶å¤¹ä¸­
export $(python -c 'import xfastertransformer as xft; print(xft.get_env())')
python examples/web_demo/ChatGLM.py \
                      --dtype=bf16 \
                      --token_path=${TOKEN_PATH} \
                      --model_path=${MODEL_PATH}
```

## æœåŠ¡

### vLLM
vllm-xfté¡¹ç›®åˆ›å»ºäº†vLLMçš„ä¸€ä¸ªåˆ†æ”¯ç‰ˆæœ¬ï¼Œè¯¥ç‰ˆæœ¬é›†æˆäº†xFasterTransformeråç«¯ä»¥æé«˜æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒäº†ä¸å®˜æ–¹vLLMå¤§å¤šæ•°åŠŸèƒ½çš„å…¼å®¹æ€§ã€‚è¯¦ç»†ä¿¡æ¯è¯·å‚è€ƒ[æ­¤é“¾æ¥](serving/vllm-xft.md)ã€‚

#### Install
```bash
pip install vllm-xft
```
***æ³¨æ„ï¼šè¯·ä¸è¦åœ¨ç¯å¢ƒä¸­åŒæ—¶å®‰è£… `vllm-xft` å’Œ `vllm` ã€‚è™½ç„¶åŒ…åä¸åŒï¼Œä½†å®é™…ä¸Šå®ƒä»¬ä¼šäº’ç›¸è¦†ç›–ã€‚***

#### å…¼å®¹OpenAI-APIçš„æœåŠ¡
***æ³¨æ„ï¼šéœ€è¦é¢„åŠ è½½ `libiomp5`ï¼***
```bash
# é€šè¿‡ä»¥ä¸‹å‘½ä»¤æˆ–æ‰‹åŠ¨è®¾ç½® LD_PRELOAD=libiomp5.so é¢„åŠ è½½ libiomp5.so
export $(python -c 'import xfastertransformer as xft; print(xft.get_env())')

python -m vllm.entrypoints.openai.api_server \
        --model ${MODEL_PATH} \
        --tokenizer ${TOKEN_PATH} \
        --dtype bf16 \
        --kv-cache-dtype fp16 \
        --served-model-name xft \
        --port 8000 \
        --trust-remote-code
```
å¯¹äºåˆ†å¸ƒå¼æ¨¡å¼ï¼Œè¯·ä½¿ç”¨ `python -m vllm.entrypoints.slave` ä½œä¸ºä»èŠ‚ç‚¹ï¼Œå¹¶ç¡®ä¿ä»èŠ‚ç‚¹çš„å‚æ•°ä¸ä¸»èŠ‚ç‚¹ä¸€è‡´ã€‚
```bash
# é€šè¿‡ä»¥ä¸‹å‘½ä»¤æˆ–æ‰‹åŠ¨è®¾ç½® LD_PRELOAD=libiomp5.so é¢„åŠ è½½ libiomp5.so
export $(python -c 'import xfastertransformer as xft; print(xft.get_env())')

OMP_NUM_THREADS=48 mpirun \
        -n 1 numactl --all -C 0-47 -m 0 \
          python -m vllm.entrypoints.openai.api_server \
            --model ${MODEL_PATH} \
            --tokenizer ${TOKEN_PATH} \
            --dtype bf16 \
            --kv-cache-dtype fp16 \
            --served-model-name xft \
            --port 8000 \
            --trust-remote-code \
        : -n 1 numactl --all -C 48-95 -m 1 \
          python -m vllm.entrypoints.slave \
            --dtype bf16 \
            --model ${MODEL_PATH} \
            --kv-cache-dtype fp16
```

### FastChat
xFasterTransformer æ˜¯ [FastChat](https://github.com/lm-sys/FastChat)çš„å®˜æ–¹æ¨ç†åç«¯ã€‚è¯¦ç»†ä¿¡æ¯è¯·å‚è€ƒ [FastChat ä¸­çš„ xFasterTransformer](https://github.com/lm-sys/FastChat/blob/main/docs/xFasterTransformer.md) å’Œ [FastChat æœåŠ¡](https://github.com/lm-sys/FastChat/blob/main/docs/openai_api.md)ã€‚

### MLServer
[MLServer æœåŠ¡ç¤ºä¾‹](serving/mlserver/README.md) æ”¯æŒ REST å’Œ gRPC æ¥å£ï¼Œå¹¶å…·æœ‰è‡ªé€‚åº”æ‰¹å¤„ç†åŠŸèƒ½ï¼Œå¯å³æ—¶å°†æ¨ç†è¯·æ±‚åˆ†ç»„ã€‚

## [æ€§èƒ½æµ‹è¯•](benchmark/README.md)

æä¾›çš„Benchmarkè„šæœ¬å¯å¿«é€Ÿè·å¾—æ¨¡å‹æ¨ç†æ€§èƒ½ã€‚
- [å‡†å¤‡æ¨¡å‹](#prepare-model).
- å®‰è£…ä¾èµ–é¡¹ï¼ŒåŒ…æ‹¬ oneCCL å’Œ python ä¾èµ–é¡¹ã€‚
- è¿›å…¥ `benchmark` æ–‡ä»¶å¤¹å¹¶è¿è¡Œ `run_benchmark.sh`ã€‚æ›´å¤šä¿¡æ¯è¯·å‚é˜… [Benchmark README](benchmark/README.md)ã€‚

**å¤‡æ³¨!!!**: ç³»ç»Ÿå’Œ CPU é…ç½®å¯èƒ½ä¸åŒã€‚ä¸ºè·å¾—æœ€ä½³æ€§èƒ½ï¼Œè¯·å°è¯•æ ¹æ®æµ‹è¯•ç¯å¢ƒä¿®æ”¹ OMP_NUM_THREADSã€æ•°æ®ç±»å‹å’Œå†…å­˜èŠ‚ç‚¹æ•°ï¼ˆä½¿ç”¨ `numactl -H` æ£€æŸ¥å†…å­˜èŠ‚ç‚¹ï¼‰ã€‚

## æŠ€æœ¯æ”¯æŒ

- xFasterTransformer é‚®ä»¶: xft.maintainer@intel.com
- xFasterTransformer [å¾®ä¿¡](https://github.com/intel/xFasterTransformer/wiki)


## è®ºæ–‡å‘è¡¨
- ICLR'2024 on practical ML for limited/low resource settings: [Distributed Inference Performance Optimization for LLMs on CPUs](https://arxiv.org/abs/2407.00029)
- ICML'2024 on Foundation Models in the Wild: [Inference Performance Optimization for Large Language Models on CPUs](https://arxiv.org/abs/2407.07304)
- IEEE ICSESS 2024: All-in-one Approach for Large Language Models Inference

å¦‚æœä½ è§‰å¾—xFTå¯¹ä½ çš„ç ”ç©¶æœ‰å¸®åŠ©ï¼Œè¯·å¼•ç”¨:
```latex
@article{he2024distributed,
  title={Distributed Inference Performance Optimization for LLMs on CPUs},
  author={He, Pujiang and Zhou, Shan and Li, Changqing and Huang, Wenhuan and Yu, Weifei and Wang, Duyi and Meng, Chen and Gui, Sheng},
  journal={arXiv preprint arXiv:2407.00029},
  year={2024}
}
```
and
```latex
@inproceedings{he2024inference,
  title={Inference Performance Optimization for Large Language Models on CPUs},
  author={He, Pujiang and Zhou, Shan and Huang, Wenhuan and Li, Changqing and Wang, Duyi and Guo, Bin and Meng, Chen and Gui, Sheng and Yu, Weifei and Xie, Yi},
  booktitle={ICML 2024 Workshop on Foundation Models in the Wild}
}
```

## é—®é¢˜ä¸å›ç­”

- ***é—®***: xFasterTransformer å¯ä»¥åœ¨ IntelÂ® Coreâ„¢ CPU ä¸Šè¿è¡Œå—ï¼Ÿ   
***ç­”***: ä¸å¯ä»¥ã€‚xFasterTransformer éœ€è¦ AMX å’Œ AVX512 æŒ‡ä»¤é›†çš„æ”¯æŒï¼Œè€ŒIntelÂ® Coreâ„¢ CPUä¸æ”¯æŒè¿™äº›æŒ‡ä»¤é›†ã€‚

- ***é—®***: xFasterTransformer å¯ä»¥åœ¨ Windows ç³»ç»Ÿä¸Šè¿è¡Œå—ï¼Ÿ  
***ç­”***: ä¸æ”¯æŒ Windowsï¼Œæ‰€æœ‰å…¼å®¹æ€§æµ‹è¯•éƒ½åªåœ¨ Linux ä¸Šè¿›è¡Œï¼Œå› æ­¤å»ºè®®ä½¿ç”¨ Linuxã€‚

- ***é—®***: é€šè¿‡ oneAPI å®‰è£…äº†æœ€æ–°ç‰ˆæœ¬çš„ oneCCL åï¼Œåœ¨å¤šè¿›ç¨‹æ¨¡å¼ä¸‹è¿è¡Œæ—¶ï¼Œä¸ºä»€ä¹ˆç¨‹åºä¼šå¡æ­»æˆ–å‡ºé”™ï¼Ÿ  
***ç­”***: è¯·å°è¯•å°† oneAPI é™çº§åˆ° 2023.x æˆ–æ›´ä½ç‰ˆæœ¬ï¼Œæˆ–ä½¿ç”¨æä¾›çš„è„šæœ¬ä»æºä»£ç å®‰è£… oneCCLã€‚

- ***é—®***: ä¸ºä»€ä¹ˆä½¿ç”¨ä¸¤ä¸ª CPU è¿è¡Œç¨‹åºçš„æ€§èƒ½æ¯”ä½¿ç”¨å•ä¸ª CPU è¿è¡Œç¨‹åºçš„æ€§èƒ½è¦ä½å¾—å¤šï¼Ÿ  
***ç­”***: ä»¥è¿™ç§æ–¹å¼è¿è¡Œä¼šå¯¼è‡´ç¨‹åºè¿›è¡Œè®¸å¤šä¸å¿…è¦çš„è·¨CPUé€šä¿¡ï¼Œä¸¥é‡å½±å“æ€§èƒ½ã€‚å¦‚æœéœ€è¦è·¨CPUéƒ¨ç½²ï¼Œå¯è€ƒè™‘åœ¨å¤šè¿›ç¨‹æ¨¡å¼ä¸‹è¿è¡Œï¼Œåœ¨æ¯ä¸ªCPUä¸Šéƒ¨ç½²ä¸€ä¸ªè¿›ç¨‹ã€‚

- ***é—®***:ä»¥å•è¿›ç¨‹è¿è¡Œæ—¶æ€§èƒ½æ­£å¸¸ï¼Œä½†ä¸ºä»€ä¹ˆä½¿ç”¨ MPI è¿è¡Œå¤šè¿›ç¨‹æ€§èƒ½å¾ˆæ…¢ï¼ŒCPU åˆ©ç”¨ç‡å¾ˆä½ï¼Ÿ   
***ç­”***:è¿™æ˜¯å› ä¸ºé€šè¿‡ MPI å¯åŠ¨çš„ç¨‹åºè¯»å–çš„æ˜¯ `OMP_NUM_THREADS=1`ï¼Œæ— æ³•ä»ç¯å¢ƒä¸­æ­£ç¡®è·å–ç›¸åº”çš„å€¼ã€‚æœ‰å¿…è¦æ ¹æ®å®é™…æƒ…å†µæ‰‹åŠ¨è®¾ç½® `OMP_NUM_THREADS` çš„å€¼ã€‚

- ***é—®***: ä¸ºä»€ä¹ˆåœ¨è½¬æ¢å·²æ”¯æŒçš„æ¨¡å‹æ—¶ä»ä¼šé‡åˆ°é”™è¯¯ï¼Ÿ  
***ç­”***: å°è¯•å°† `transformer` é™çº§åˆ°åˆé€‚çš„ç‰ˆæœ¬ã€‚è¿™æ˜¯å› ä¸ºä¸åŒç‰ˆæœ¬çš„ Transformer å¯èƒ½ä¼šæ›´æ”¹æŸäº›å˜é‡çš„åç§°ã€‚

- ***é—®***ï¼šç¼–è¯‘æ—¶é‡åˆ°é”™è¯¯ï¼Œæç¤ºæ‰¾ä¸åˆ° `mkl.h`ï¼Œæˆ‘è¯¥æ€ä¹ˆåŠï¼Ÿ  
***ç­”***ï¼šè¯·æ£€æŸ¥ `3rdparty/` ç›®å½•ä¸‹çš„ `onednn` æ–‡ä»¶å¤¹æ˜¯å¦ä¸ºç©ºã€‚å¦‚æœä¸ºç©ºï¼Œè¯·å°†å…¶åˆ é™¤å¹¶é‡æ–°è¿è¡Œ CMakeã€‚æ­¤å¤–ï¼Œå¦‚æœ `3rdparty/mkl/` æ–‡ä»¶å¤¹å†…ä»…åŒ…å« `local` ç›®å½•ï¼Œè¯·å°† `mkl/local/*` ä¸­çš„æ‰€æœ‰å†…å®¹ç§»åŠ¨åˆ° `mkl/` ç›®å½•ä¸‹ã€‚
