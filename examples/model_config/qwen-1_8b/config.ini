[qwen]
model_name = /data/models/Qwen-1_8B-Chat/
head_num = 16
kv_head_num = 16
size_per_head = 128
inter_size = 5504
max_pos_seq_len = 8192
num_layer = 24
rms_norm_eps = 1e-6
layernorm_type = pre_layernorm
activation_type = silu
has_post_decoder_layernorm = 1
vocab_size = 151936
seq_length = 8192
start_id = None
end_id = 151643
pad_id = 151643
weight_data_type = fp16
use_logn_attn = True
use_dynamic_ntk = True
