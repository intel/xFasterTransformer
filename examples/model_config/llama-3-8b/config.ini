[llama]
model_name = /data/models/Meta-Llama-3-8B-Instruct
head_num = 32
kv_head_num = 8
hidden_size = 4096
size_per_head = 128
inter_size = 14336
max_pos_seq_len = 8192
num_layer = 32
layernorm_eps = 1e-05
layernorm_type = pre_layernorm
activation_type = silu
rope_theta = 500000.0
scaling_factor = 1.0
rope_type = null
has_post_decoder_layernorm = 1
vocab_size = 128256
start_id = 128000
end_id = 128001
weight_data_type = fp16

