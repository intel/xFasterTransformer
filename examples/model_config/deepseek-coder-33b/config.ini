[llama]
model_name = /data/models/deepseek-coder-33b-instruct
head_num = 56
kv_head_num = 8
size_per_head = 128
inter_size = 19200
max_pos_seq_len = 16384
num_layer = 62
layernorm_eps = 1e-06
layernorm_type = pre_layernorm
activation_type = silu
has_post_decoder_layernorm = 1
vocab_size = 32256
start_id = 32013
end_id = 32021
rope_type = linear
rope_theta = 100000
scaling_factor = 4.0
weight_data_type = fp16
